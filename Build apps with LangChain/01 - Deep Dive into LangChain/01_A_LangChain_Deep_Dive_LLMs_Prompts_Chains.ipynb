{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8e1476ee-4cac-4c75-939e-c6872292a6cc",
      "metadata": {
        "id": "8e1476ee-4cac-4c75-939e-c6872292a6cc"
      },
      "source": [
        "# Deep Dive into LangChain\n",
        "## LLMs, Prompt Templates, Caching, Streaming, Chains\n",
        "\n",
        "This notebook uses the latest versions of the OpenAI and LangChain libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7b118e8-d6e2-4deb-b621-c2f90bc8bd83",
      "metadata": {
        "id": "d7b118e8-d6e2-4deb-b621-c2f90bc8bd83"
      },
      "outputs": [],
      "source": [
        "pip install -r ./requirements.txt -q"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72695b0b-0069-484d-9edd-9b75faaa8307",
      "metadata": {
        "id": "72695b0b-0069-484d-9edd-9b75faaa8307"
      },
      "source": [
        "Download [requirements.txt](https://drive.google.com/file/d/1UpURYL9kqjXfe9J8o-_Dq5KJTbQpzMef/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "57410c7b-edb3-4843-98b5-d71dd2d5bf4e",
      "metadata": {
        "id": "57410c7b-edb3-4843-98b5-d71dd2d5bf4e",
        "outputId": "2df9efe6-5de1-4e82-9cc9-f77c28af47b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m974.0/974.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.7/314.7 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.7/142.7 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "pip install --upgrade -q langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8cc5a125-4737-4ab8-ba14-7d4af2685e1b",
      "metadata": {
        "id": "8cc5a125-4737-4ab8-ba14-7d4af2685e1b",
        "outputId": "4038b9e8-95bc-4f44-e428-96a8a498629f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.5/325.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "pip install --upgrade -q openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e5444b4-384d-440e-a9a3-2b4c3b5f837c",
      "metadata": {
        "id": "4e5444b4-384d-440e-a9a3-2b4c3b5f837c"
      },
      "outputs": [],
      "source": [
        "# pip show openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "06fd9c8e-65ce-4422-b733-ee14876b2614",
      "metadata": {
        "id": "06fd9c8e-65ce-4422-b733-ee14876b2614",
        "outputId": "6cded085-4176-49af-a25e-f0481371b3c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Package(s) not found: langchain\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "pip show langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain_openai"
      ],
      "metadata": {
        "id": "nRICk-gpdFXo",
        "outputId": "c8f3e465-469b-44f9-f7dd-9d98517998e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "nRICk-gpdFXo",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.1.8-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: langchain-core<0.3,>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.2.5)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (1.34.0)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain_openai)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (6.0.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.66 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (0.1.77)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (23.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (2.7.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (8.3.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (0.27.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (4.12.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.31.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.26.0->langchain_openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.26.0->langchain_openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.26.0->langchain_openai) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.26.0->langchain_openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.26.0->langchain_openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.2.2->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.66->langchain-core<0.3,>=0.2.2->langchain_openai) (3.10.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2.2->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2.2->langchain_openai) (2.18.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.0.7)\n",
            "Installing collected packages: tiktoken, langchain_openai\n",
            "Successfully installed langchain_openai-0.1.8 tiktoken-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ee9a618-d423-4cce-8e32-ed937698c0d5",
      "metadata": {
        "id": "2ee9a618-d423-4cce-8e32-ed937698c0d5"
      },
      "source": [
        "### Python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e052148c-1dd2-4cd2-8878-fd90e58dabfe",
      "metadata": {
        "id": "e052148c-1dd2-4cd2-8878-fd90e58dabfe",
        "outputId": "62e0c89f-7c0c-44a0-f914-f27098f17a97"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "# loading the API Keys from .env\n",
        "load_dotenv(find_dotenv(), override=True)\n",
        "\n",
        "# os.environ.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "654afaf9-00bc-4d4d-99ab-907055d4139c",
      "metadata": {
        "id": "654afaf9-00bc-4d4d-99ab-907055d4139c"
      },
      "source": [
        "## Chat Models: GPT-3.5 Turbo and GPT-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "a2eb526e-4105-4301-b931-f285f258035b",
      "metadata": {
        "scrolled": true,
        "id": "a2eb526e-4105-4301-b931-f285f258035b",
        "outputId": "29df371f-1314-4493-f44c-3c6d79567b8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-d832932ed8b6>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# invoking the llm (running the prompt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Explain quantum mechanics in one sentence.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gpt-3.5-turbo'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         return cast(\n\u001b[1;32m    169\u001b[0m             \u001b[0mChatGeneration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    171\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    597\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    598\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     async def agenerate_prompt(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                     \u001b[0mrun_managers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m         flattened_outputs = [\n\u001b[1;32m    458\u001b[0m             \u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[list-item]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    444\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m                 results.append(\n\u001b[0;32m--> 446\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    447\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m                 result = self._generate(\n\u001b[0m\u001b[1;32m    672\u001b[0m                     \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0mmessage_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_message_dicts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_chat_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    604\u001b[0m         \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeout\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mNotGiven\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNOT_GIVEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 606\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    607\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1238\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m         )\n\u001b[0;32m-> 1240\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    919\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 921\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    922\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1003\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mretries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1005\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1006\u001b[0m                     \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1051\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1053\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1054\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1003\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mretries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1005\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1006\u001b[0m                     \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1051\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1053\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1054\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "llm = ChatOpenAI(openai_api_key=userdata.get('OPENAI_API_KEY'))\n",
        "\n",
        "# invoking the llm (running the prompt)\n",
        "output = llm.invoke('Explain quantum mechanics in one sentence.', model='gpt-3.5-turbo', temperature=0.1)\n",
        "print(output.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54b5b306-6905-4c58-82ef-17cc9be9777d",
      "metadata": {
        "scrolled": true,
        "id": "54b5b306-6905-4c58-82ef-17cc9be9777d"
      },
      "outputs": [],
      "source": [
        "# help(ChatOpenAI)  # see the llm constructor arguments with its defaults"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da2baa8e-8440-4c16-a1bb-91f8ff0ac556",
      "metadata": {
        "id": "da2baa8e-8440-4c16-a1bb-91f8ff0ac556",
        "outputId": "425bb2f5-dc63-47fa-feb1-644d68b3ec68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quantenmechanik beschreibt das Verhalten von Teilchen auf subatomarer Ebene, indem sie sowohl Wellen- als auch Teilcheneigenschaften besitzen.\n"
          ]
        }
      ],
      "source": [
        "# using Chat Completions API Messages: System, Assistant and Human\n",
        "from langchain.schema import(\n",
        "    SystemMessage,\n",
        "    AIMessage,\n",
        "    HumanMessage\n",
        ")\n",
        "messages = [\n",
        "    SystemMessage(content='You are a physicist and respond only in German.'),\n",
        "    HumanMessage(content='Explain quantum mechanics in one sentence.')\n",
        "]\n",
        "\n",
        "output = llm.invoke(messages)\n",
        "print(output.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "919e837c-d307-4165-ad99-551a2bee6cf5",
      "metadata": {
        "id": "919e837c-d307-4165-ad99-551a2bee6cf5"
      },
      "source": [
        "## Caching LLM Responses"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7379171e-2081-4cc1-a834-89ebff34e84c",
      "metadata": {
        "id": "7379171e-2081-4cc1-a834-89ebff34e84c"
      },
      "source": [
        "### 1. In-Memory Cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a73a4e4-d351-4f4c-b149-151fe3b2c878",
      "metadata": {
        "id": "7a73a4e4-d351-4f4c-b149-151fe3b2c878"
      },
      "outputs": [],
      "source": [
        "from langchain.globals import set_llm_cache\n",
        "from langchain_openai import OpenAI\n",
        "llm = OpenAI(model_name='gpt-3.5-turbo-instruct')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d356c163-afd3-4534-9947-69e088f16913",
      "metadata": {
        "id": "d356c163-afd3-4534-9947-69e088f16913"
      },
      "outputs": [],
      "source": [
        "from langchain.cache import InMemoryCache\n",
        "set_llm_cache(InMemoryCache())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20232db3-97a5-498e-805f-fbcb24d14584",
      "metadata": {
        "id": "20232db3-97a5-498e-805f-fbcb24d14584",
        "outputId": "97efeca2-7757-43e1-b266-14eac68b6359"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 7.42 ms, sys: 0 ns, total: 7.42 ms\n",
            "Wall time: 700 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"\\n\\nWhy did the banana go to the doctor?\\n\\nBecause it wasn't peeling well! \""
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "prompt = 'Tell a me a joke that a toddler can understand.'\n",
        "llm.invoke(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18d58e8e-c8b3-4719-b4ad-a3d03bfb7655",
      "metadata": {
        "id": "18d58e8e-c8b3-4719-b4ad-a3d03bfb7655",
        "outputId": "9ec4c7c4-fcfc-417f-9f0a-696e6d82cb01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 120 µs, sys: 110 µs, total: 230 µs\n",
            "Wall time: 233 µs\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"\\n\\nWhy did the banana go to the doctor?\\n\\nBecause it wasn't peeling well! \""
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "llm.invoke(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6269939d-b92f-406a-a53f-9a7bf561c5a2",
      "metadata": {
        "id": "6269939d-b92f-406a-a53f-9a7bf561c5a2"
      },
      "source": [
        "### 2. SQLite Caching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9271fe21-1ca3-4cb2-8599-9a368071482d",
      "metadata": {
        "id": "9271fe21-1ca3-4cb2-8599-9a368071482d"
      },
      "outputs": [],
      "source": [
        "from langchain.cache import SQLiteCache\n",
        "set_llm_cache(SQLiteCache(database_path=\".langchain.db\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a13f4281-b6c7-4a46-ab08-b02528f4111e",
      "metadata": {
        "id": "a13f4281-b6c7-4a46-ab08-b02528f4111e",
        "outputId": "c5fa0e77-d96b-4360-dd4b-34d82b57c123"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 18.1 ms, sys: 0 ns, total: 18.1 ms\n",
            "Wall time: 833 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'\\n\\nWhy was the math book sad?\\nBecause it had too many problems.'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "# First request (not in cache, takes longer)\n",
        "llm.invoke(\"Tell me a joke\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3fe8a61-0812-427b-bdae-06adc67988f6",
      "metadata": {
        "id": "e3fe8a61-0812-427b-bdae-06adc67988f6",
        "outputId": "75668a59-9efa-490f-9bc0-f6e25a703611"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 35.9 ms, sys: 39.4 ms, total: 75.3 ms\n",
            "Wall time: 74.4 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'\\n\\nWhy was the math book sad?\\nBecause it had too many problems.'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "# Second request (cached, faster)\n",
        "llm.invoke(\"Tell me a joke\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f56aa044-5842-4d50-b3e9-0207421df1cf",
      "metadata": {
        "id": "f56aa044-5842-4d50-b3e9-0207421df1cf"
      },
      "source": [
        "## LLM Streaming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "174adda0-ce4a-40bb-8a28-31abc494f266",
      "metadata": {
        "scrolled": true,
        "id": "174adda0-ce4a-40bb-8a28-31abc494f266",
        "outputId": "89595be4-049c-46f7-8f15-e41ca52a2fc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(Verse 1)\n",
            "Under the silver moonlight, the raven takes flight,\n",
            "Across the midnight sky, with wings so dark and wide.\n",
            "A mystical creature, a messenger of the night,\n",
            "Guiding me through the shadows, with its piercing eyes.\n",
            "\n",
            "(Pre-Chorus)\n",
            "Oh moon, shining bright, your glow illuminates the night,\n",
            "And the raven, in its grace, dances with the starry haze.\n",
            "They tell tales of secrets, hidden in the lunar phase,\n",
            "In this rock anthem, their legend we embrace.\n",
            "\n",
            "(Chorus)\n",
            "Moon and Raven, so wild and untamed,\n",
            "A cosmic alliance, forever unchained.\n",
            "Their spirits entwined, in the darkest domain,\n",
            "This rock song echoes their eternal refrain.\n",
            "\n",
            "(Verse 2)\n",
            "The moon, a celestial queen, reflecting ancient dreams,\n",
            "A beacon in the darkness, igniting hope it seems.\n",
            "The raven, a rebel, defying rules and norms,\n",
            "Together they transcend, beyond earthly forms.\n",
            "\n",
            "(Bridge)\n",
            "In the lunar abyss, they find solace and bliss,\n",
            "An unbreakable bond, a tale seldom amiss.\n",
            "They navigate the heavens, defying gravity's call,\n",
            "Through moonlit melodies, they conquer it all.\n",
            "\n",
            "(Chorus)\n",
            "Moon and Raven, so wild and untamed,\n",
            "A cosmic alliance, forever unchained.\n",
            "Their spirits entwined, in the darkest domain,\n",
            "This rock song echoes their eternal refrain.\n",
            "\n",
            "(Guitar Solo)\n",
            "\n",
            "(Verse 3)\n",
            "As the night unfolds, their legend only grows,\n",
            "With every beat of the drum, their story overflows.\n",
            "The moon and raven, an enigmatic pair,\n",
            "Their essence resonates, in the hearts that dare.\n",
            "\n",
            "(Outro)\n",
            "Moon and Raven, forever intertwined,\n",
            "In this rock anthem, their spirits will find,\n",
            "A place in our souls, where their legends reside,\n",
            "Their cosmic symphony, forever amplified.\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI()\n",
        "prompt = 'Write a rock song about the Moon and a Raven.'\n",
        "print(llm.invoke(prompt).content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a94d50a4-60ca-4009-a051-b83cf70c7a45",
      "metadata": {
        "scrolled": true,
        "id": "a94d50a4-60ca-4009-a051-b83cf70c7a45",
        "outputId": "2ced6162-15b0-407a-a282-fadc00f19997"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(Verse 1)\n",
            "In the dead of night, beneath the silver light,\n",
            "The moon's glow illuminates the sky so bright,\n",
            "A raven takes flight, with feathers black as coal,\n",
            "Its piercing eyes, a story yet untold.\n",
            "\n",
            "(Pre-Chorus)\n",
            "Through the shadows, they dance, their destinies entwined,\n",
            "The moon and the raven, an alliance undefined,\n",
            "Together they soar, in the depths of the night,\n",
            "A tale of darkness and pure lunar might.\n",
            "\n",
            "(Chorus)\n",
            "Moon and raven, a celestial duet,\n",
            "Unleashing power, in every silhouette,\n",
            "They rule the night, a symphony untamed,\n",
            "Through the darkness, their legacy proclaimed.\n",
            "\n",
            "(Verse 2)\n",
            "On lunar beams, the raven finds its way,\n",
            "Across the night sky, where secrets gently sway,\n",
            "The moon whispers secrets with a tranquil glow,\n",
            "As the raven listens, their bond continues to grow.\n",
            "\n",
            "(Bridge)\n",
            "In the moonlit haze, they share a cosmic dance,\n",
            "Their spirits intertwining in a lunar trance,\n",
            "Together they conquer, the night's eternal reign,\n",
            "Their union unbreakable, forever to remain.\n",
            "\n",
            "(Chorus)\n",
            "Moon and raven, a celestial duet,\n",
            "Unleashing power, in every silhouette,\n",
            "They rule the night, a symphony untamed,\n",
            "Through the darkness, their legacy proclaimed.\n",
            "\n",
            "(Solo)\n",
            "\n",
            "(Chorus)\n",
            "Moon and raven, a celestial duet,\n",
            "Unleashing power, in every silhouette,\n",
            "They rule the night, a symphony untamed,\n",
            "Through the darkness, their legacy proclaimed.\n",
            "\n",
            "(Outro)\n",
            "In the realm where shadows dwell,\n",
            "The moon and raven forever cast their spell,\n",
            "Their bond unbreakable, their spirits ever free,\n",
            "In the realm of rock, where legends will forever be."
          ]
        }
      ],
      "source": [
        "for chunk in llm.stream(prompt):\n",
        "    print(chunk.content, end='', flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84844cfe-7469-47a6-841c-0fc31d8cdb49",
      "metadata": {
        "id": "84844cfe-7469-47a6-841c-0fc31d8cdb49"
      },
      "source": [
        "## PromptTemplates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34389e61-d08b-440a-8a6f-96e99b5bee8c",
      "metadata": {
        "id": "34389e61-d08b-440a-8a6f-96e99b5bee8c",
        "outputId": "93bacbc9-7089-4a25-8c35-a356abc5be5d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'You are an experienced virologist.\\nWrite a few sentences about the following virus \"hiv\" in german.'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Define a template for the prompt\n",
        "template = '''You are an experienced virologist.\n",
        "Write a few sentences about the following virus \"{virus}\" in {language}.'''\n",
        "\n",
        "# Create a PromptTemplate object from the template\n",
        "prompt_template = PromptTemplate.from_template(template=template)\n",
        "\n",
        "# Fill in the variable: virus and language\n",
        "prompt = prompt_template.format(virus='hiv', language='german')\n",
        "prompt  # Returns the generated prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "602e2f25-4ba8-4a48-b116-4589af56f9fc",
      "metadata": {
        "id": "602e2f25-4ba8-4a48-b116-4589af56f9fc",
        "outputId": "1a5505d5-e880-489a-f70c-888d756a189e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HIV, das humane Immundefizienz-Virus, ist ein Retrovirus, das das menschliche Immunsystem angreift. Es wird hauptsächlich durch ungeschützten Geschlechtsverkehr, den Austausch von infizierten Nadeln oder von Mutter zu Kind während der Schwangerschaft, Geburt oder Stillzeit übertragen. HIV kann zu AIDS führen, einer schweren Erkrankung, bei der das Immunsystem geschwächt ist und der Körper anfällig für verschiedene Infektionen und Krankheiten wird. Obwohl es keine Heilung für HIV gibt, kann die rechtzeitige Diagnose und Behandlung das Fortschreiten der Krankheit verlangsamen und die Lebensqualität der Betroffenen verbessern.\n"
          ]
        }
      ],
      "source": [
        "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
        "output = llm.invoke(prompt)\n",
        "print(output.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8084261-b4c6-47e6-8a38-03f339f1f5c5",
      "metadata": {
        "id": "d8084261-b4c6-47e6-8a38-03f339f1f5c5"
      },
      "source": [
        "## ChatPromptTemplates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fab3930-930d-4c45-a3d8-a6b213d77270",
      "metadata": {
        "id": "3fab3930-930d-4c45-a3d8-a6b213d77270",
        "outputId": "29075352-b951-4412-a2b9-0444c1c9fa1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SystemMessage(content='You respond only in the JSON format.'), HumanMessage(content='Top 5 countries in World by population.')]\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain_core.messages import SystemMessage\n",
        "\n",
        "# Create a chat template with system and human messages\n",
        "chat_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        SystemMessage(content='You respond only in the JSON format.'),\n",
        "        HumanMessagePromptTemplate.from_template('Top {n} countries in {area} by population.')\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Fill in the specific values for n and area\n",
        "messages = chat_template.format_messages(n='5', area='World')\n",
        "print(messages)  # Outputs the formatted chat messages\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a768271-ff8d-4d4c-a7c3-21ebed1abd6a",
      "metadata": {
        "scrolled": true,
        "id": "4a768271-ff8d-4d4c-a7c3-21ebed1abd6a",
        "outputId": "0f25f040-95ce-49d2-9c0d-5f4ae8423d63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"data\": [\n",
            "    {\n",
            "      \"country\": \"China\",\n",
            "      \"population\": 1444216107\n",
            "    },\n",
            "    {\n",
            "      \"country\": \"India\",\n",
            "      \"population\": 1393409038\n",
            "    },\n",
            "    {\n",
            "      \"country\": \"United States\",\n",
            "      \"population\": 332915073\n",
            "    },\n",
            "    {\n",
            "      \"country\": \"Indonesia\",\n",
            "      \"population\": 276361783\n",
            "    },\n",
            "    {\n",
            "      \"country\": \"Pakistan\",\n",
            "      \"population\": 225199937\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "llm = ChatOpenAI()\n",
        "output = llm.invoke(messages)\n",
        "print(output.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb2eb160-df97-4229-8115-2e8864b400d3",
      "metadata": {
        "id": "eb2eb160-df97-4229-8115-2e8864b400d3"
      },
      "source": [
        "## Simple Chains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e2aef22-26c3-4e8c-8bbd-99383b6052e9",
      "metadata": {
        "id": "0e2aef22-26c3-4e8c-8bbd-99383b6052e9",
        "outputId": "76f61eb1-4af3-4e94-97fa-a7c55b10fc6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are an experience virologist.\n",
            "Write a few sentences about the following virus \"HSV\" in Spanish.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "llm = ChatOpenAI()\n",
        "template = '''You are an experience virologist.\n",
        "Write a few sentences about the following virus \"{virus}\" in {language}.'''\n",
        "prompt_template = PromptTemplate.from_template(template=template)\n",
        "\n",
        "chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt_template,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "output = chain.invoke({'virus': 'HSV', 'language': 'Spanish'})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "161f079d-a38d-48f5-8466-9b2c185446f4",
      "metadata": {
        "id": "161f079d-a38d-48f5-8466-9b2c185446f4",
        "outputId": "ce9b9edf-eee6-40f1-d0cc-ad52c34b3493"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'virus': 'HSV', 'language': 'Spanish', 'text': 'El virus del herpes simple (HSV) es un virus altamente contagioso que afecta a los seres humanos. Se divide en dos tipos: HSV-1 y HSV-2. El HSV-1 generalmente se asocia con infecciones orales, como el herpes labial, mientras que el HSV-2 está relacionado con infecciones genitales. Ambos tipos de HSV pueden causar brotes recurrentes a lo largo de la vida de una persona, aunque los síntomas pueden variar desde leves hasta graves. Actualmente no existe una cura para el HSV, pero hay medicamentos disponibles que pueden ayudar a controlar los síntomas y reducir la frecuencia de los brotes.'}\n"
          ]
        }
      ],
      "source": [
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c23cc0a0-6b02-4007-9192-80f00d7576a4",
      "metadata": {
        "id": "c23cc0a0-6b02-4007-9192-80f00d7576a4",
        "outputId": "c330762e-ffe0-42f8-8bcc-f26e4fb115da"
      },
      "outputs": [
        {
          "name": "stdin",
          "output_type": "stream",
          "text": [
            "Enter Country:  Spain\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mWhat is the capital of Spain?. List the top 3 places to visit in that city. Use bullet points\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "The capital of Spain is Madrid. Here are the top 3 places to visit in Madrid:\n",
            "\n",
            "- The Royal Palace: Explore this magnificent 18th-century palace, which is the official residence of the Spanish Royal Family. Admire the stunning architecture, luxurious rooms, and beautiful gardens.\n",
            "- Prado Museum: Immerse yourself in art at the Prado Museum, one of the world's finest art galleries. Discover masterpieces by renowned Spanish artists such as Velázquez, Goya, and El Greco.\n",
            "- Retiro Park: Escape the hustle and bustle of the city and relax in Retiro Park. This vast green oasis offers peaceful gardens, a boating lake, fountains, and even a beautiful glass palace.\n"
          ]
        }
      ],
      "source": [
        "template = 'What is the capital of {country}?. List the top 3 places to visit in that city. Use bullet points'\n",
        "prompt_template = PromptTemplate.from_template(template=template)\n",
        "\n",
        "# Initialize an LLMChain with the ChatOpenAI model and the prompt template\n",
        "chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt_template,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "country = input('Enter Country: ')\n",
        "\n",
        "# Invoke the chain with specific virus and language values\n",
        "output = chain.invoke(country)\n",
        "print(output['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "963536b8-76e7-4681-bb0d-8309795e2fa3",
      "metadata": {
        "id": "963536b8-76e7-4681-bb0d-8309795e2fa3"
      },
      "source": [
        "## Sequential Chains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9e711bc-8bcf-45ce-b993-52e42fb650cd",
      "metadata": {
        "scrolled": true,
        "id": "f9e711bc-8bcf-45ce-b993-52e42fb650cd",
        "outputId": "67cd2202-957e-485f-8bab-fcd76dcbaab6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3mSure! Here's an example of a function that implements the concept of linear regression in Python:\n",
            "\n",
            "```python\n",
            "import numpy as np\n",
            "\n",
            "def linear_regression(x, y):\n",
            "    # Calculate the number of data points\n",
            "    n = len(x)\n",
            "    \n",
            "    # Calculate the mean of x and y\n",
            "    x_mean = np.mean(x)\n",
            "    y_mean = np.mean(y)\n",
            "    \n",
            "    # Calculate the slope (beta1) and intercept (beta0) of the regression line\n",
            "    numerator = np.sum((x - x_mean) * (y - y_mean))\n",
            "    denominator = np.sum((x - x_mean) ** 2)\n",
            "    beta1 = numerator / denominator\n",
            "    beta0 = y_mean - beta1 * x_mean\n",
            "    \n",
            "    # Return the slope and intercept of the regression line\n",
            "    return beta1, beta0\n",
            "```\n",
            "\n",
            "This function takes two arrays, `x` and `y`, as inputs. `x` represents the independent variable and `y` represents the dependent variable. The function calculates the slope (`beta1`) and intercept (`beta0`) of the regression line using the least squares method. It then returns these values.\n",
            "\n",
            "To use this function, you can pass your data points to it like this:\n",
            "\n",
            "```python\n",
            "x = np.array([1, 2, 3, 4, 5])\n",
            "y = np.array([2, 4, 5, 4, 5])\n",
            "\n",
            "slope, intercept = linear_regression(x, y)\n",
            "print(\"Slope:\", slope)\n",
            "print(\"Intercept:\", intercept)\n",
            "```\n",
            "\n",
            "This will output:\n",
            "\n",
            "```\n",
            "Slope: 0.7\n",
            "Intercept: 2.2\n",
            "```\n",
            "\n",
            "These values represent the equation of the regression line: `y = 0.7x + 2.2`.\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3mThe Python function `linear_regression` is an implementation of simple linear regression, which is a fundamental statistical approach used to model the relationship between a single independent variable (denoted as `x`) and a dependent variable (denoted as `y`). The main goal of this function is to find the best-fitting straight line (also known as the regression line) that describes this relationship. The equation of this line is `y = beta1 * x + beta0`, where `beta1` is the slope of the line, and `beta0` is the y-intercept.\n",
            "\n",
            "The implementation details of the `linear_regression` function are explained below:\n",
            "\n",
            "1. **Imports and Dependencies**: The function uses NumPy (imported as `np`), a popular library for numerical computing in Python. NumPy provides efficient ways to handle arrays and mathematical operations, making it an ideal choice for implementing linear regression.\n",
            "\n",
            "2. **Parameters**: The function accepts two parameters, `x` and `y`, which are NumPy arrays of the same length. The elements of `x` represent the independent variable values, and the elements of `y` represent the dependent variable values.\n",
            "\n",
            "3. **Number of Data Points**: The function starts by determining `n`, the number of data points, which is the length of the `x` array (and equivalently, the `y` array).\n",
            "\n",
            "4. **Mean Calculation**: It calculates the mean (average) values of both `x` and `y` using `np.mean()`. These mean values are crucial for determining the slope and intercept of the regression line.\n",
            "\n",
            "5. **Slope (Beta1) and Intercept (Beta0) Calculation**: \n",
            "    - The slope (`beta1`) of the regression line measures the change in the dependent variable (`y`) for a one-unit change in the independent variable (`x`). It is calculated by dividing the sum of the products of differences between each `x` value and the mean of `x` with corresponding differences between each `y` value and the mean of `y`, by the sum of the squares of differences between each `x` value and the mean of `x`. Mathematically, it is expressed as: \n",
            "    \\[ beta1 = \\frac{\\sum ((x_i - x_{mean}) * (y_i - y_{mean}))}{\\sum ((x_i - x_{mean})^2)} \\]\n",
            "    \n",
            "    - The intercept (`beta0`) of the regression line represents the value of `y` when `x` is zero. It is calculated using the relationship between the means of `x` and `y`, and the slope (`beta1`). Mathematically: \n",
            "    \\[ beta0 = y_{mean} - beta1 * x_{mean} \\]\n",
            "\n",
            "6. **Return Values**: Finally, the function returns the calculated values of the slope (`beta1`) and intercept (`beta0`) of the regression line. \n",
            "\n",
            "To use this function, the user needs to provide arrays of `x` and `y` values, and the function will output the slope and intercept of the line that fits best to these points in the context of least squares fitting. These outputs essentially define the linear equation that best describes the relationship between the independent and dependent variables in the given dataset. The given implementation is a clear example for teaching or understanding the basics of simple linear regression without relying on higher-level APIs or libraries that obfuscate these fundamental calculations.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chains import LLMChain, SimpleSequentialChain\n",
        "\n",
        "# Initialize the first ChatOpenAI model (gpt-3.5-turbo) with specific temperature\n",
        "llm1 = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.5)\n",
        "\n",
        "# Define the first prompt template\n",
        "prompt_template1 = PromptTemplate.from_template(\n",
        "    template='You are an experienced scientist and Python programmer. Write a function that implements the concept of {concept}.'\n",
        ")\n",
        "# Create an LLMChain using the first model and the prompt template\n",
        "chain1 = LLMChain(llm=llm1, prompt=prompt_template1)\n",
        "\n",
        "# Initialize the second ChatOpenAI model (gpt-4-turbo) with specific temperature\n",
        "llm2 = ChatOpenAI(model_name='gpt-4-turbo-preview', temperature=1.2)\n",
        "\n",
        "# Define the second prompt template\n",
        "prompt_template2 = PromptTemplate.from_template(\n",
        "    template='Given the Python function {function}, describe it as detailed as possible.'\n",
        ")\n",
        "# Create another LLMChain using the second model and the prompt template\n",
        "chain2 = LLMChain(llm=llm2, prompt=prompt_template2)\n",
        "\n",
        "# Combine both chains into a SimpleSequentialChain\n",
        "overall_chain = SimpleSequentialChain(chains=[chain1, chain2], verbose=True)\n",
        "\n",
        "# Invoke the overall chain with the concept \"linear regression\"\n",
        "output = overall_chain.invoke('linear regression')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b365ca5e-84f6-484f-829b-1ba9ff9798e0",
      "metadata": {
        "id": "b365ca5e-84f6-484f-829b-1ba9ff9798e0",
        "outputId": "c9922dd5-57ef-4bd4-a588-106aa7b01ea1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Python function `linear_regression` is an implementation of simple linear regression, which is a fundamental statistical approach used to model the relationship between a single independent variable (denoted as `x`) and a dependent variable (denoted as `y`). The main goal of this function is to find the best-fitting straight line (also known as the regression line) that describes this relationship. The equation of this line is `y = beta1 * x + beta0`, where `beta1` is the slope of the line, and `beta0` is the y-intercept.\n",
            "\n",
            "The implementation details of the `linear_regression` function are explained below:\n",
            "\n",
            "1. **Imports and Dependencies**: The function uses NumPy (imported as `np`), a popular library for numerical computing in Python. NumPy provides efficient ways to handle arrays and mathematical operations, making it an ideal choice for implementing linear regression.\n",
            "\n",
            "2. **Parameters**: The function accepts two parameters, `x` and `y`, which are NumPy arrays of the same length. The elements of `x` represent the independent variable values, and the elements of `y` represent the dependent variable values.\n",
            "\n",
            "3. **Number of Data Points**: The function starts by determining `n`, the number of data points, which is the length of the `x` array (and equivalently, the `y` array).\n",
            "\n",
            "4. **Mean Calculation**: It calculates the mean (average) values of both `x` and `y` using `np.mean()`. These mean values are crucial for determining the slope and intercept of the regression line.\n",
            "\n",
            "5. **Slope (Beta1) and Intercept (Beta0) Calculation**: \n",
            "    - The slope (`beta1`) of the regression line measures the change in the dependent variable (`y`) for a one-unit change in the independent variable (`x`). It is calculated by dividing the sum of the products of differences between each `x` value and the mean of `x` with corresponding differences between each `y` value and the mean of `y`, by the sum of the squares of differences between each `x` value and the mean of `x`. Mathematically, it is expressed as: \n",
            "    \\[ beta1 = \\frac{\\sum ((x_i - x_{mean}) * (y_i - y_{mean}))}{\\sum ((x_i - x_{mean})^2)} \\]\n",
            "    \n",
            "    - The intercept (`beta0`) of the regression line represents the value of `y` when `x` is zero. It is calculated using the relationship between the means of `x` and `y`, and the slope (`beta1`). Mathematically: \n",
            "    \\[ beta0 = y_{mean} - beta1 * x_{mean} \\]\n",
            "\n",
            "6. **Return Values**: Finally, the function returns the calculated values of the slope (`beta1`) and intercept (`beta0`) of the regression line. \n",
            "\n",
            "To use this function, the user needs to provide arrays of `x` and `y` values, and the function will output the slope and intercept of the line that fits best to these points in the context of least squares fitting. These outputs essentially define the linear equation that best describes the relationship between the independent and dependent variables in the given dataset. The given implementation is a clear example for teaching or understanding the basics of simple linear regression without relying on higher-level APIs or libraries that obfuscate these fundamental calculations.\n"
          ]
        }
      ],
      "source": [
        "print(output['output'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2a8cee1-18fa-4068-a234-defdb7ed2b2e",
      "metadata": {
        "id": "d2a8cee1-18fa-4068-a234-defdb7ed2b2e"
      },
      "outputs": [],
      "source": [
        "pip install -q langchain_experimental"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c9b9413-9d79-47e4-8464-40cb3756700e",
      "metadata": {
        "id": "6c9b9413-9d79-47e4-8464-40cb3756700e",
        "outputId": "e5c256aa-2982-420c-f601-1be963801a17"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Python REPL can execute arbitrary code. Use with caution.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'[13, 26, 39, 52, 65, 78, 91]\\n'"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_experimental.utilities import PythonREPL\n",
        "python_repl = PythonREPL()\n",
        "python_repl.run('print([n for n in range(1, 100) if n % 13 == 0])')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9973644c-65a3-44ac-ac9e-1be39950acdd",
      "metadata": {
        "id": "9973644c-65a3-44ac-ac9e-1be39950acdd",
        "outputId": "7b8415dd-3be6-4539-fc78-b0849869de42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mTo solve this, I need to calculate the factorial of 12 first, then find its square root, and finally format the result to display it with 4 decimal points. I can use the `math` module in Python for both the factorial and square root calculations.\n",
            "Action: Python_REPL\n",
            "Action Input: import math\n",
            "print(f\"{math.sqrt(math.factorial(12)):.4f}\")\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m21886.1052\n",
            "\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
            "Final Answer: 21886.1052\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'Calculate the square root of the factorial of 12 and display it with 4 decimal points',\n",
              " 'output': '21886.1052'}"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_experimental.agents.agent_toolkits import create_python_agent\n",
        "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Initialize the ChatOpenAI model with gpt-4-turbo and a temperature of 0\n",
        "llm = ChatOpenAI(model='gpt-4-turbo-preview', temperature=0)\n",
        "\n",
        "# Create a Python agent using the ChatOpenAI model and a PythonREPLTool\n",
        "agent_executor = create_python_agent(\n",
        "    llm=llm,\n",
        "    tool=PythonREPLTool(),\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Invoke the agent\n",
        "prompt = 'Calculate the square root of the factorial of 12 and display it with 4 decimal points'\n",
        "agent_executor.invoke(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a1046e3-42dd-41a1-be25-8f48454f92f6",
      "metadata": {
        "id": "8a1046e3-42dd-41a1-be25-8f48454f92f6",
        "outputId": "f6548a11-0709-419d-8f85-17d59f66ff60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mI need to calculate 5.1 raised to the power of 7.3 to get the answer.\n",
            "Action: Python_REPL\n",
            "Action Input: print(5.1 ** 7.3)\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m146306.05007233328\n",
            "\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
            "Final Answer: 146306.05007233328\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "response = agent_executor.invoke('What is the answer to 5.1 ** 7.3?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af92f7d0-06c4-475e-9deb-8ab0352050ab",
      "metadata": {
        "id": "af92f7d0-06c4-475e-9deb-8ab0352050ab",
        "outputId": "297bde6a-4248-4067-c69e-2b4244a01550"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input': 'What is the answer to 5.1 ** 7.3?', 'output': '146306.05007233328'}"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e936c363-5720-443b-b4f9-29d0b5263536",
      "metadata": {
        "id": "e936c363-5720-443b-b4f9-29d0b5263536",
        "outputId": "af50ff85-bd5d-4e78-8fb2-8d312b7713fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is the answer to 5.1 ** 7.3?\n"
          ]
        }
      ],
      "source": [
        "print(response['input'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1accfc09-b6c0-4c0a-8cc6-54590d081f86",
      "metadata": {
        "id": "1accfc09-b6c0-4c0a-8cc6-54590d081f86",
        "outputId": "4cd06a13-492e-4230-affc-cb2e42e7ed0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "146306.05007233328\n"
          ]
        }
      ],
      "source": [
        "print(response['output'])\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}